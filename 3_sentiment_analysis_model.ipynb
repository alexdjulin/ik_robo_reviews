{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore two models from the BERT family to perform sentiment analysis on reviews. \n",
    "- The first model has already been fine-tuned for this task and we can run inference on it\n",
    "- We fine-tune the second model on our dataset to try to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Initialize tqdm with pandas\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# import helpers module\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "SEP = 100 * '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from pickle/data_processed.pkl.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[name] All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[brand] Amazon\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[categories] Electronics,iPad & Tablets,All Tablets,Fire Tablets,Tablets,Computers & Tablets\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[reviews.rating] 5.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[review] Amazon - Fire Kids Edition\n",
      "I got this for my kid, because it is tailored for kids. They liked it and it worked.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = helpers.load_pickled_dataset('pickle/data_processed.pkl')\n",
    "\n",
    "helpers.print_random_product_sheet(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map sentiment based on rating\n",
    "The review sentiment can take 3 values: negative, neutral or positive. Let's add a column and map the sentiment corresponding to the rating, for future inference, fine-tuning and comparison with the predictions. For that, we need to reduce the 5 star-labels to 3 sentiments (negative, neutral, positive), thus generating some imprecisions.\n",
    "\n",
    "Since most people usually rate a product when they are happy or not, we can assume that the neutral sentiment is not very common and we will map it to 3 stars only, assuming that 1-2 starts is a negative comment and 4-5 stars a positive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50109 entries, 0 to 50108\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   name               50109 non-null  object \n",
      " 1   brand              50109 non-null  object \n",
      " 2   categories         50109 non-null  object \n",
      " 3   reviews.rating     50109 non-null  float64\n",
      " 4   review             50109 non-null  object \n",
      " 5   reviews.sentiment  50109 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviews.sentiment\n",
       "positive    0.915365\n",
       "neutral     0.045920\n",
       "negative    0.038716\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most people rate a product when they are happy or not, so we can assume that the neutral sentiment is not very common\n",
    "review_mapping = {\n",
    "    1: 'negative',\n",
    "    2: 'negative',\n",
    "    3: 'neutral',\n",
    "    4: 'positive',\n",
    "    5: 'positive'\n",
    "}\n",
    "\n",
    "data['reviews.sentiment'] = data['reviews.rating'].map(review_mapping)\n",
    "\n",
    "print(data.info())\n",
    "data['reviews.sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we notice how unbalanced our dataset is, with 91% of positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 1: Use a fine-tuned model\n",
    "In this first approach, we use a HuggingFace model from the BERT family, that has already been fine-tuned for sentiment analysis. We can therefore run inference immediately, once our reviews are tokenized.\n",
    "\n",
    "I did some research and decided to use [bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you), wich is a BERT-family model fine-tuned on product reviews.\n",
    "\n",
    "- This model has been fine-tuned on product reviews, which makes it more suitable for handling the nuances and specific vocabulary commonly found in reviews (e.g., product features, usability, satisfaction).\n",
    "\n",
    "- Multilingual Support: Since it's multilingual, it can handle a wide range of languages if your Amazon reviews dataset includes non-English text. This makes it versatile if you encounter reviews in German, Spanish, or other languages.\n",
    "\n",
    "- Performance: BERT models are known for their good performance on sentiment analysis tasks, and this particular model is specifically optimized for product review sentiment, likely making it more accurate in distinguishing subtle sentiment shifts in review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input and run inference on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9db69dbeca9458c8eea5f91675a888f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a591bc4b2be4cd79ebb97afeb514d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54818e39028a466796c50ab9ba339d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c77a3ddd47349f090e84bcf6c516fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778d6aebfed84ecf997c9de6d0225749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Example function to predict sentiment for a single review\n",
    "def predict_sentiment(review_text):\n",
    "    inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = torch.argmax(probabilities).item()\n",
    "    \n",
    "    # we want to output 3 sentiments: negative, neutral, positive\n",
    "    # for sentiments 1 and 3, we check side probabilities to decide\n",
    "    if sentiment == 0:\n",
    "        return 'negative'\n",
    "    elif sentiment == 1:\n",
    "        return 'negative' if probabilities[0][0] > probabilities[0][2] else 'neutral'\n",
    "    elif sentiment == 2:\n",
    "        return 'neutral'\n",
    "    elif sentiment == 3:\n",
    "        return 'positive' if probabilities[0][4] > probabilities[0][2] else 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "review = \"This product is good.\"\n",
    "print(f\"Sentiment: {predict_sentiment(review)}\")\n",
    "\n",
    "review = \"This is the worst product ever.\"\n",
    "print(f\"Sentiment: {predict_sentiment(review)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on the full dataset is very time consuming. Let's start with a subset of the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of the dataset\n",
    "test_data = data.head(1000)\n",
    "\n",
    "# predict on the dataset's reviews and store the result into a new column\n",
    "test_data['predicted.sentiment'] = test_data['review'].progress_apply(predict_sentiment)\n",
    "\n",
    "print(test_data.info())\n",
    "test_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# let's compute an accuracy score to compare the predicted sentiment with the actual review\n",
    "accuracy = (test_data['reviews.sentiment'] == test_data['predicted.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not so bad. Let's run the accuracy on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50174/50174 [06:06<00:00, 137.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict on the dataset's reviews and store the result into a new column\n",
    "data['predicted.sentiment'] = data['review'].progress_apply(predict_sentiment)\n",
    "\n",
    "# print the accuracy score\n",
    "accuracy = (test_data['reviews.sentiment'] == test_data['predicted.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is sensibly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 2: Fine-Tune a Transformer on our dataset to increase accuracy\n",
    "\n",
    "Our reviews are very specific to Amazon and amazon products. Let's fine tune a transformer model to do sentiment analysis on our dataset. This way it will be able to classify reviews without the star-rating.\n",
    "\n",
    "### Model choice\n",
    "As said, the star-rating system is not very accurate, due to the reduction of 5 star labels to 3 sentiments (negative, neutral, positive). So we should priorize a model that is fast (if we want it to run in real time for the demo) and don't need a high accuracy. Let's use [distilbert-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased) for that, which is a lightweight version of the BERT-family models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "40087 40087\n",
      "10022 10022\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and labels\n",
    "X = data['review'].to_list()\n",
    "y = data['reviews.sentiment'].to_list()\n",
    "\n",
    "# convert labels to integers, 0 for negative, 1 for neutral, 2 for positive\n",
    "y = [0 if sentiment == 'negative' else 1 if sentiment == 'neutral' else 2 for sentiment in y]\n",
    "print(set(y))  # check result\n",
    "\n",
    "# Split data into train and test sets (80-20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "# print the number of samples in each set\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=\"max_length\", max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "print(train_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset Class\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert each tokenized item to tensor and add label as well\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        # item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        # item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long).to(device)  # Adjust labels as needed\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and validation datasets\n",
    "train_dataset = EncodedDataset(train_encodings, y_train)\n",
    "val_dataset = EncodedDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics to track\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-Tune the Model\n",
    "# trainer.train()\n",
    "\n",
    "# Save model\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# model_path = f\"models/distilbert_sa_{timestamp}\"\n",
    "# model.save_pretrained(model_path)\n",
    "# print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = \"models/distilbert_sa_20241015_143753\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='627' max='627' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [627/627 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.0426701307296753, 'eval_accuracy': 0.11454799441229295, 'eval_runtime': 37.7486, 'eval_samples_per_second': 265.493, 'eval_steps_per_second': 16.61}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch prediction on reviews and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  reviews.ft\n",
      "0  Great device for reading. Definately pricey.\\n...           2\n",
      "1  Excellent Kindle\\nThe best Kindle ever, for me...           2\n",
      "2  Love it\\nI absolutely love this reader. The bi...           2\n",
      "3  Good kindle\\nI always use it when i read ebook...           2\n",
      "4  So much to love, but slippery\\nLove bigger scr...           2\n"
     ]
    }
   ],
   "source": [
    "# Function to perform inference in batches\n",
    "def predict_in_batches(model, texts, batch_size=32):\n",
    "    predictions = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            # Tokenize the batch of texts\n",
    "            encodings = tokenizer(batch_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Run inference and get logits\n",
    "            outputs = model(**encodings)\n",
    "            batch_preds = torch.argmax(outputs.logits, dim=1)  # Get the predicted class\n",
    "            predictions.extend(batch_preds.cpu().numpy())  # Move to CPU to avoid GPU memory overload\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Run the prediction function on your review column\n",
    "data['reviews.ft'] = predict_in_batches(model, data['review'].tolist(), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50109 entries, 0 to 50108\n",
      "Data columns (total 8 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   name                  50109 non-null  object \n",
      " 1   brand                 50109 non-null  object \n",
      " 2   categories            50109 non-null  object \n",
      " 3   reviews.rating        50109 non-null  float64\n",
      " 4   review                50109 non-null  object \n",
      " 5   reviews.sentiment     50109 non-null  object \n",
      " 6   reviews.ft            50109 non-null  int64  \n",
      " 7   reviews.ft.sentiment  50109 non-null  object \n",
      "dtypes: float64(1), int64(1), object(6)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data['reviews.ft.sentiment'] = data['reviews.ft'].map({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare predictions over real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = (data['reviews.sentiment'] == data['reviews.ft.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we are getting a much higher accuracy, considering that we run the model on the same data we used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Sentiment Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('pickle/data_sentiment_analysis.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
