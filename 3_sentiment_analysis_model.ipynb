{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore two models from the BERT family to perform sentiment analysis on reviews. \n",
    "- The first model has already been fine-tuned for this task and we can run inference on it\n",
    "- We fine-tune the second model on our dataset to try to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 15:09:21.684769: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 15:09:21.684866: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 15:09:21.686539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 15:09:21.696544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 15:09:23.202811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # Initialize tqdm with pandas\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# import helpers module\n",
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers)\n",
    "\n",
    "SEP = 100 * '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from pickle/data_processed.pkl.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[name] AmazonBasics AAA Performance Alkaline Batteries (36 Count)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[brand] Amazonbasics\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[categories] AA,AAA,Health,Electronics,Health & Household,Camcorder Batteries,Camera & Photo,Batteries,Household Batteries,Robot Check,Accessories,Camera Batteries,Health and Beauty,Household Supplies,Batteries & Chargers,Health, Household & Baby Care,Health Personal Care\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[reviews.rating] 5.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[reviews.numHelpful] 0.0\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[review] So far so good, had issues with duracells leaking, not noticing any less performance and no leakers :)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data = helpers.load_pickled_dataset('pickle/data_processed.pkl')\n",
    "\n",
    "helpers.print_random_product_sheet(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map sentiment based on rating\n",
    "The review sentiment can take 3 values: negative, neutral or positive. Let's add a column and map the sentiment corresponding to the rating, for future inference, fine-tuning and comparison with the predictions. For that, we need to reduce the 5 star-labels to 3 sentiments (negative, neutral, positive), thus generating some imprecisions.\n",
    "\n",
    "Since most people usually rate a product when they are happy or not, we can assume that the neutral sentiment is not very common and we will map it to 3 stars only, assuming that 1-2 starts is a negative comment and 4-5 stars a positive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59046 entries, 0 to 59045\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   name                59046 non-null  object \n",
      " 1   brand               59046 non-null  object \n",
      " 2   categories          59046 non-null  object \n",
      " 3   reviews.rating      59046 non-null  float64\n",
      " 4   reviews.numHelpful  59046 non-null  float64\n",
      " 5   review              59046 non-null  object \n",
      " 6   reviews.sentiment   59046 non-null  object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 3.2+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviews.sentiment\n",
       "positive    0.916150\n",
       "neutral     0.044525\n",
       "negative    0.039325\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most people rate a product when they are happy or not, so we can assume that the neutral sentiment is not very common\n",
    "review_mapping = {\n",
    "    1: 'negative',\n",
    "    2: 'negative',\n",
    "    3: 'neutral',\n",
    "    4: 'positive',\n",
    "    5: 'positive'\n",
    "}\n",
    "\n",
    "data['reviews.sentiment'] = data['reviews.rating'].map(review_mapping)\n",
    "\n",
    "print(data.info())\n",
    "data['reviews.sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we notice how unbalanced our dataset is, with 91% of positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 1: Use a fine-tuned model\n",
    "In this first approach, we use a HuggingFace model from the BERT family, that has already been fine-tuned for sentiment analysis. We can therefore run inference immediately, once our reviews are tokenized.\n",
    "\n",
    "I did some research and decided to use [bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you), wich is a BERT-family model fine-tuned on product reviews.\n",
    "\n",
    "- This model has been fine-tuned on product reviews, which makes it more suitable for handling the nuances and specific vocabulary commonly found in reviews (e.g., product features, usability, satisfaction).\n",
    "\n",
    "- Multilingual Support: Since it's multilingual, it can handle a wide range of languages if your Amazon reviews dataset includes non-English text. This makes it versatile if you encounter reviews in German, Spanish, or other languages.\n",
    "\n",
    "- Performance: BERT models are known for their good performance on sentiment analysis tasks, and this particular model is specifically optimized for product review sentiment, likely making it more accurate in distinguishing subtle sentiment shifts in review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input and run inference on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Example function to predict sentiment for a single review\n",
    "def predict_sentiment(review_text):\n",
    "    inputs = tokenizer(review_text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = torch.argmax(probabilities).item()\n",
    "    \n",
    "    # we want to output 3 sentiments: negative, neutral, positive\n",
    "    # for sentiments 1 and 3, we check side probabilities to decide\n",
    "    if sentiment == 0:\n",
    "        return 'negative'\n",
    "    elif sentiment == 1:\n",
    "        return 'negative' if probabilities[0][0] > probabilities[0][2] else 'neutral'\n",
    "    elif sentiment == 2:\n",
    "        return 'neutral'\n",
    "    elif sentiment == 3:\n",
    "        return 'positive' if probabilities[0][4] > probabilities[0][2] else 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n",
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "review = \"This product is good.\"\n",
    "print(f\"Sentiment: {predict_sentiment(review)}\")\n",
    "\n",
    "review = \"This is the worst product ever.\"\n",
    "print(f\"Sentiment: {predict_sentiment(review)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the model on the full dataset is very time consuming. Let's start with a subset of the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of the dataset\n",
    "test_data = data.head(1000)\n",
    "\n",
    "# predict on the dataset's reviews and store the result into a new column\n",
    "test_data['predicted.sentiment'] = test_data['review'].progress_apply(predict_sentiment)\n",
    "\n",
    "print(test_data.info())\n",
    "test_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# let's compute an accuracy score to compare the predicted sentiment with the actual review\n",
    "accuracy = (test_data['reviews.sentiment'] == test_data['predicted.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not so bad. Let's run the accuracy on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50174/50174 [06:06<00:00, 137.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict on the dataset's reviews and store the result into a new column\n",
    "data['predicted.sentiment'] = data['review'].progress_apply(predict_sentiment)\n",
    "\n",
    "# print the accuracy score\n",
    "accuracy = (test_data['reviews.sentiment'] == test_data['predicted.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is sensibly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 2: Fine-Tune a Transformer on our dataset to increase accuracy\n",
    "\n",
    "Our reviews are very specific to Amazon and amazon products. Let's fine tune a transformer model to do sentiment analysis on our dataset. This way it will be able to classify reviews without the star-rating.\n",
    "\n",
    "### Model choice\n",
    "As said, the star-rating system is not very accurate, due to the reduction of 5 star labels to 3 sentiments (negative, neutral, positive). So we should priorize a model that is fast (if we want it to run in real time for the demo) and don't need a high accuracy. Let's use [distilbert-base-uncased](https://huggingface.co/distilbert/distilbert-base-uncased) for that, which is a lightweight version of the BERT-family models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bd380d2f4745f8ab259015c12fd258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf77845e504fa5ae05913c6aa77700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65895ad46e6540aa90aea41b383d0ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d6afa834f54637aefd969a98ee6bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4473ada6ab944acbcfa825cef12b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "41332 41332\n",
      "8857 8857\n",
      "8857 8857\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and labels\n",
    "X = data['review'].to_list()\n",
    "y = data['reviews.sentiment'].to_list()\n",
    "\n",
    "# convert labels to integers, 0 for negative, 1 for neutral, 2 for positive\n",
    "y = [0 if sentiment == 'negative' else 1 if sentiment == 'neutral' else 2 for sentiment in y]\n",
    "print(set(y))  # check result\n",
    "\n",
    "# Split data into train and validation sets (60-40 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Split validation set into validation and test sets (50-50 split)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, stratify=y_val, random_state=42)\n",
    "\n",
    "# print the number of samples in each set\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))\n",
    "print(len(X_test), len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the data\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=\"max_length\", max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=\"max_length\", max_length=128)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "print(train_encodings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset Class\n",
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert each tokenized item to tensor and add label as well\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and validation datasets\n",
    "train_dataset = EncodedDataset(train_encodings, y_train)\n",
    "val_dataset = EncodedDataset(val_encodings, y_val)\n",
    "test_dataset = EncodedDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6579' max='6579' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6579/6579 20:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.357900</td>\n",
       "      <td>0.354357</td>\n",
       "      <td>0.915381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.355755</td>\n",
       "      <td>0.915381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.356083</td>\n",
       "      <td>0.915381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/distilbert_sa_20241016_115241\n"
     ]
    }
   ],
   "source": [
    "# Define metrics to track\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-Tune the Model\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"models/distilbert_sa_{timestamp}\"\n",
    "model.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = \"models/distilbert_sa_20241016_115241\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 0.3560831546783447\n",
      "eval_accuracy: 0.9153805215540181\n",
      "eval_runtime: 26.9117\n",
      "eval_samples_per_second: 279.284\n",
      "eval_steps_per_second: 17.465\n",
      "epoch: 3.0\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.35402196645736694\n",
      "test_accuracy: 0.9153917786350938\n",
      "test_runtime: 27.6758\n",
      "test_samples_per_second: 271.609\n",
      "test_steps_per_second: 16.982\n"
     ]
    }
   ],
   "source": [
    "# Run predictions on test set\n",
    "test_results = trainer.predict(test_dataset=test_dataset)\n",
    "\n",
    "# Extract and print accuracy from test results\n",
    "for key, value in test_results.metrics.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a final accuracy of 91.5%, we can say that our fine-tuned model is doing a decent job at assigning one of the 3 sentiments to reviews, some customer reviews being very short or ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch prediction on reviews and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference in batches\n",
    "def predict_in_batches(model, texts, batch_size=32):\n",
    "    predictions = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "            # Tokenize the batch of texts\n",
    "            encodings = tokenizer(batch_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            # Run inference and get logits\n",
    "            outputs = model(**encodings)\n",
    "            batch_preds = torch.argmax(outputs.logits, dim=1)  # Get the predicted class\n",
    "            predictions.extend(batch_preds.cpu().numpy())  # Move to CPU to avoid GPU memory overload\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Run the prediction function on your review column\n",
    "data['reviews.ft'] = predict_in_batches(model, data['review'].tolist(), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59046 entries, 0 to 59045\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   name                  59046 non-null  object \n",
      " 1   brand                 59046 non-null  object \n",
      " 2   categories            59046 non-null  object \n",
      " 3   reviews.rating        59046 non-null  float64\n",
      " 4   reviews.numHelpful    59046 non-null  float64\n",
      " 5   review                59046 non-null  object \n",
      " 6   reviews.sentiment     59046 non-null  object \n",
      " 7   reviews.ft            59046 non-null  int64  \n",
      " 8   reviews.ft.sentiment  59046 non-null  object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data['reviews.ft.sentiment'] = data['reviews.ft'].map({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare predictions over real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = (data['reviews.sentiment'] == data['reviews.ft.sentiment']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we are getting a much higher accuracy, considering that we run the model on the same data we used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Sentiment Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('pickle/data_sentiment_analysis.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
